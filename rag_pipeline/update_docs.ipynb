{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bde016f-3c82-48fb-9a2e-dbc1ceab3616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt # Original had -U\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f142702c-f87f-4fe5-8790-b506d29cb41a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Packages"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import markdownify\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc07f90-1808-4401-86e4-5ab52654f877",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Variables"
    }
   },
   "outputs": [],
   "source": [
    "MAX_CONTENT_SIZE = 5 * 1024 * 1024  # 5MB per document\n",
    "MAX_BATCH_SIZE = 100 * 1024 * 1024  # Flush every 100MB\n",
    "URL_PARALLELISM = 50\n",
    "\n",
    "MAX_DOCUMENTS = None\n",
    "DATABRICKS_SITEMAP_URL = r\"https://docs.databricks.com/aws/en/sitemap.xml\"\n",
    "\n",
    "with open(\"names.yaml\", \"r\") as file:\n",
    "    names = yaml.safe_load(file)\n",
    "TABLE_NAME = names.get(\"table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0028c14-29a2-4068-8ddf-b15577b8f637",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Knowledge base table"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS {names.get(\"table_name\")} (\n",
    "    id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "    url STRING,\n",
    "    content STRING\n",
    "  ) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2b71aa-7f26-4f7f-9616-6dd57505b711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Getting databricks docs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9160142-2274-419b-a364-185ef12e847f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Support functions to web scrapping"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch and convert HTML to Markdown\n",
    "def fetch_and_parse(url, session):\n",
    "    resp = session.get(url, stream=True, timeout=10)\n",
    "    if resp.status_code != 200:\n",
    "        return url, resp.status_code\n",
    "    content = b\"\"\n",
    "    for chunk in resp.iter_content(chunk_size=8192):\n",
    "        content += chunk\n",
    "        if len(content) > MAX_CONTENT_SIZE:\n",
    "            return url, \"TOO_LARGE\"\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    article = soup.find(\"article\")\n",
    "    if not article:\n",
    "        return url, None\n",
    "    html_str = article.prettify()\n",
    "    if len(html_str.encode(\"utf-8\")) > MAX_CONTENT_SIZE:\n",
    "        return url, \"TOO_LARGE\"\n",
    "    markdown = markdownify.markdownify(html_str, heading_style=\"ATX\")\n",
    "    if len(markdown.encode(\"utf-8\")) > MAX_CONTENT_SIZE:\n",
    "        return url, \"TOO_LARGE\"\n",
    "    return url, markdown\n",
    "\n",
    "# Fetch sitemap URLs\n",
    "def fetch_urls(session, max_documents=None):\n",
    "    response = session.get(DATABRICKS_SITEMAP_URL)\n",
    "    root = ET.fromstring(response.content)\n",
    "    urls = [loc.text for loc in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
    "    return urls[:max_documents] if max_documents else urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e9d7dc-8a4d-49e4-9e05-5950dc38ded8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup shared retry-enabled HTTP session"
    }
   },
   "outputs": [],
   "source": [
    "pending_results = []\n",
    "accumulated_size, total_processed, total_skipped, chunk_idx = 0, 0, 0, 0\n",
    "\n",
    "shared_session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=3, status_forcelist=[429])\n",
    "adapter = HTTPAdapter(max_retries=retries, pool_maxsize=URL_PARALLELISM)\n",
    "shared_session.mount(\"http://\", adapter)\n",
    "shared_session.mount(\"https://\", adapter)\n",
    "\n",
    "print(f'Downloading Databricks documentation to {TABLE_NAME}, this can take a few minutes...')\n",
    "urls = fetch_urls(shared_session, MAX_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08a0e3c-0b91-4b0e-8c2b-167972b2526c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Downloading and saving docs"
    }
   },
   "outputs": [],
   "source": [
    "def flush_to_delta(data):\n",
    "    global chunk_idx\n",
    "    df = pd.DataFrame(data, columns=[\"url\", \"content\"])\n",
    "    df = df[df[\"content\"].notnull() & (df[\"content\"] != \"TOO_LARGE\")]\n",
    "    if not df.empty:\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        mode = \"overwrite\" if chunk_idx == 0 else \"append\"\n",
    "        (spark_df.write\n",
    "                .mode(mode)\n",
    "                .format(\"delta\")\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .saveAsTable(names.get(\"table_name\")))\n",
    "        chunk_idx += 1\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=URL_PARALLELISM) as executor:\n",
    "\n",
    "    futures = {executor.submit(fetch_and_parse, url, shared_session): url for url in urls}\n",
    "    for future in as_completed(futures):\n",
    "        url, text = future.result()\n",
    "\n",
    "        if text and text != \"TOO_LARGE\":\n",
    "            size = len(text.encode(\"utf-8\"))\n",
    "            accumulated_size += size\n",
    "            pending_results.append((url, text))\n",
    "        else:\n",
    "            print(f\"SKIPPED ({text}): {url}\")\n",
    "            total_skipped += 1\n",
    "\n",
    "        if accumulated_size >= MAX_BATCH_SIZE or len(pending_results) >= 250:\n",
    "            print(f\"Flushing {len(pending_results)} rows (~{accumulated_size / 1024 / 1024:.1f} MB)\")\n",
    "            flush_to_delta(pending_results)\n",
    "            pending_results = []\n",
    "            accumulated_size = 0\n",
    "        total_processed += 1\n",
    "\n",
    "# Final flush\n",
    "if pending_results:\n",
    "    print(f\"Final flush with {len(pending_results)} rows\")\n",
    "    flush_to_delta(pending_results)\n",
    "print(f'Total skipped: {total_skipped}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1055378587390782,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "update_docs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
