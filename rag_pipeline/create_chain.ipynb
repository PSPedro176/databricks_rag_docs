{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd012fa1-3851-4807-9e49-08161b660269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE_YAML = False\n",
    "\n",
    "CATALOG = 'perdomo_demos'\n",
    "SCHEMA = \"rag_chatbot\"\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.databricks_documentation\"\n",
    "MODEL_NAME = f\"{CATALOG}.{SCHEMA}.databricks_docs_rag_chatbot\"\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"rag_chatbot_endpoint\"\n",
    "VS_INDEX_FULLNAME = f\"{TABLE_NAME}_vs_index\"\n",
    "\n",
    "NAMES = {\n",
    "    \"table_name\": TABLE_NAME,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    \"vs_index_fullname\": VS_INDEX_FULLNAME\n",
    "}\n",
    "\n",
    "CHAIN_CONFIG = {\n",
    "    \"databricks_resources\": {\n",
    "        \"llm_endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    },\n",
    "    \"input_example\": {\n",
    "        \"messages\": [{\"content\": \"How can I disable serverless?\", \"role\": \"user\"}]\n",
    "    },\n",
    "    \"llm_config\": {\n",
    "        \"llm_parameters\": {\"max_tokens\": 1500, \"temperature\": 0.01},\n",
    "        \"llm_prompt_template\": \"You are a trusted AI assistant that helps answer questions based only on the provided information. If you do not know the answer to a question, you truthfully say you do not know. Here is the history of the current conversation you are having with your user: {chat_history}. And here is some context which may or may not help you answer the following question: {context}.  Answer directly, do not repeat the question, do not start with something like: the answer to the question, do not add AI in front of your answer, do not say: here is the answer, do not mention the context or the question. Based on this context, answer this question: {question}\",\n",
    "        \"llm_prompt_template_variables\": [\"context\", \"chat_history\", \"question\"],\n",
    "    },\n",
    "    \"retriever_config\": {\n",
    "        \"chunk_template\": \"Passage: {chunk_text}\\n\",\n",
    "        \"data_pipeline_tag\": \"poc\",\n",
    "        \"parameters\": {\"k\": 5}, # , \"query_type\": \"ann\"\n",
    "        \"schema\": {\"chunk_text\": \"content\", \"document_uri\": \"url\", \"primary_key\": \"id\"},\n",
    "        \"vector_search_index\": VS_INDEX_FULLNAME,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ad88f6-e598-4051-897f-f7e3499e6409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt # Original had -U\n",
    "#dbutils.library.restartPython()\n",
    "\n",
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from databricks_langchain.vectorstores import DatabricksVectorSearch\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "\n",
    "if USE_YAML:\n",
    "    with open(\"names.yaml\", \"r\") as file:\n",
    "        names = yaml.safe_load(file)\n",
    "\n",
    "    TABLE_NAME = names.get(\"table_name\")\n",
    "    VECTOR_SEARCH_ENDPOINT_NAME = names.get(\"vector_search_endpoint_name\")\n",
    "    VS_INDEX_FULLNAME = names.get(\"vs_index_fullname\")\n",
    "    CHAIN_CONFIG = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')\n",
    "\n",
    "# Methods to format the docs returned by the retriever into the prompt (keep only the text from chunks)\n",
    "def format_context(docs):\n",
    "    chunk_contents = [f\"Passage: {d.page_content}\\n\" for d in docs]\n",
    "    return \"\".join(chunk_contents)\n",
    "  \n",
    "def extract_user_query_string(chat_messages_array: str):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "def extract_previous_messages(chat_messages_array):\n",
    "    messages = \"\\n\"\n",
    "    for msg in chat_messages_array[:-1]:\n",
    "        messages += (msg[\"role\"] + \": \" + msg[\"content\"] + \"\\n\")\n",
    "    return messages\n",
    "\n",
    "def combine_all_messages_for_vector_search(chat_messages_array):\n",
    "    return extract_previous_messages(chat_messages_array) + extract_user_query_string(chat_messages_array)\n",
    "\n",
    "# Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "databricks_resources = CHAIN_CONFIG.get(\"databricks_resources\")\n",
    "llm_config = CHAIN_CONFIG.get(\"llm_config\")\n",
    "retriever_config = CHAIN_CONFIG.get(\"retriever_config\")\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    endpoint=databricks_resources.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=retriever_config.get(\"vector_search_index\"),\n",
    "    columns=[\n",
    "        retriever_config.get(\"schema\").get(\"primary_key\"),\n",
    "        retriever_config.get(\"schema\").get(\"chunk_text\"),\n",
    "        retriever_config.get(\"schema\").get(\"document_uri\"),\n",
    "    ],\n",
    ").as_retriever(k=5, query_type=\"ann\")\n",
    "\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=retriever_config.get(\"schema\").get(\"primary_key\"),\n",
    "    text_column=retriever_config.get(\"schema\").get(\"chunk_text\"),\n",
    "    doc_uri=retriever_config.get(\"schema\").get(\"document_uri\")\n",
    ")\n",
    "\n",
    "# Model and prompt configuration\n",
    "prompt = PromptTemplate(\n",
    "    template=llm_config.get(\"llm_prompt_template\"),\n",
    "    input_variables=llm_config.get(\"llm_prompt_template_variables\"),\n",
    ")\n",
    "\n",
    "model = ChatDatabricks(\n",
    "    endpoint=databricks_resources.get(\"llm_endpoint_name\"),\n",
    "    extra_params=llm_config.get(\"llm_parameters\")\n",
    ")\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"context\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(combine_all_messages_for_vector_search)\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_previous_messages)\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "create_chain",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
